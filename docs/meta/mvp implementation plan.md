# MVP Implementation Plan: Hybrid Engine Backend

## Overview

This plan delivers a resilient, local-first Python backend that executes workflows through a file-based queue system with persistent state management. The implementation prioritizes API contract compliance and recovery capabilities from the ground up.

---

## Phase 0: Foundation & API Scaffolding

**Goal:** Establish project structure, data models, and a running API server that respects the frontend contract specifications.

|Component|File(s) to Create/Modify|Implementation Details & Key Code|
|---|---|---|
|**Project Setup**|`requirements.txt`, `.gitignore`|`txt<br/>fastapi==0.110.0<br/>uvicorn==0.29.0<br/>pydantic==2.6.4<br/>python-dotenv==1.0.1<br/>loguru==0.7.2<br/>`<br/>Standard Python .gitignore excluding `__pycache__/`, `.env`, `*.pyc`, `workflows/`, `queue-state.json`|
|**Configuration**|`.env`, `config.py`|`python<br/># config.py<br/>import os<br/>from dotenv import load_dotenv<br/><br/>load_dotenv()<br/><br/>class Settings:<br/> WORKFLOWS_DIR = os.getenv("WORKFLOWS_DIR", "workflows")<br/> QUEUE_STATE_FILE = os.getenv("QUEUE_STATE_FILE", "queue-state.json")<br/> MAX_PARALLEL_NODES = int(os.getenv("MAX_PARALLEL_NODES", "4"))<br/> LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")<br/><br/>settings = Settings()<br/>`<br/><br/>`.env` file:<br/>`<br/>WORKFLOWS_DIR=workflows<br/>QUEUE_STATE_FILE=queue-state.json<br/>MAX_PARALLEL_NODES=4<br/>LOG_LEVEL=INFO<br/>`|
|**Pydantic Models**|`models/__init__.py`, `models/workflow.py`|`python<br/># models/workflow.py<br/>from enum import Enum<br/>from datetime import datetime<br/>from pydantic import BaseModel, Field<br/>from typing import Dict, List, Optional<br/><br/>class JobStatus(str, Enum):<br/> PENDING = 'PENDING'<br/> RUNNING = 'RUNNING'<br/> COMPLETED = 'COMPLETED'<br/> FAILED = 'FAILED'<br/> WAITING_FOR_DEPENDENCY = 'WAITING_FOR_DEPENDENCY'<br/> STOPPED = 'STOPPED'<br/><br/>class WorkflowStep(BaseModel):<br/> id: str<br/> name: str<br/> action: str<br/> status: JobStatus<br/> dependencies: List[str] = []<br/> outputs: Optional[Dict[str, any]] = None<br/> error: Optional[str] = None<br/> startTime: Optional[datetime] = None<br/> endTime: Optional[datetime] = None<br/> # CRITICAL: Frontend contract requirements<br/> duration: Optional[str] = None # "1 min 30 sec"<br/> logs: Optional[List[str]] = None<br/> metadata: Optional[Dict[str, any]] = None<br/><br/>class Workflow(BaseModel):<br/> id: str<br/> name: str<br/> status: JobStatus<br/> steps: List[WorkflowStep]<br/> createdAt: datetime<br/> updatedAt: datetime<br/> description: Optional[str] = None<br/> tags: Optional[List[str]] = None<br/> progress: Optional[int] = Field(None, ge=0, le=100)<br/> parentId: Optional[str] = None<br/> branches: Optional[List[Dict[str, str]]] = None<br/> metrics: Optional[Dict[str, any]] = None<br/>`|
|**API Scaffolding**|`app.py`|`python<br/># app.py<br/>from fastapi import FastAPI, APIRouter<br/>from contextlib import asynccontextmanager<br/>from models.workflow import Workflow, JobStatus<br/>from datetime import datetime<br/>import uuid<br/><br/>@asynccontextmanager<br/>async def lifespan(app: FastAPI):<br/> # Startup: Will add worker initialization in Phase 1<br/> yield<br/> # Shutdown: Cleanup if needed<br/><br/>app = FastAPI(title="Hybrid Engine Backend", lifespan=lifespan)<br/>router = APIRouter(prefix="/api")<br/><br/>@router.get("/health")<br/>async def health_check():<br/> return {"status": "healthy", "timestamp": datetime.now()}<br/><br/># Stub endpoints for Phase 0 validation<br/>@router.get("/workflows")<br/>async def get_workflows() -> List[Workflow]:<br/> return []<br/><br/>@router.post("/workflows/from-template")<br/>async def create_workflow(template_id: str, params: dict) -> Workflow:<br/> # Stub implementation<br/> return Workflow(<br/> id=str(uuid.uuid4()),<br/> name=f"Workflow from {template_id}",<br/> status=JobStatus.PENDING,<br/> steps=[],<br/> createdAt=datetime.now(),<br/> updatedAt=datetime.now()<br/> )<br/><br/>app.include_router(router)<br/>`|

**Success Criteria for Phase 0:**

- ✅ `pip install -r requirements.txt` completes without errors
- ✅ `uvicorn app:app --reload` starts server successfully
- ✅ `GET http://127.0.0.1:8000/api/health` returns 200 with JSON response
- ✅ OpenAPI docs at `http://127.0.0.1:8000/docs` display correct Pydantic models
- ✅ All datetime fields show proper ISO 8601 format in documentation

---

## Phase 1: Core Execution Loop

**Goal:** Implement end-to-end workflow creation and execution with persistent state, job queuing, and background worker process.

|Component|File(s) to Create/Modify|Implementation Details & Key Code|
|---|---|---|
|**State Manager**|`services/__init__.py`, `services/state_manager.py`|`python<br/># services/state_manager.py<br/>import json<br/>from pathlib import Path<br/>from datetime import datetime<br/>from models.workflow import Workflow<br/>from config import settings<br/>from loguru import logger<br/><br/>class StateManager:<br/> def __init__(self, workflow_id: str):<br/> self.workflow_id = workflow_id<br/> self.workflow_dir = Path(settings.WORKFLOWS_DIR) / workflow_id<br/> self.state_file = self.workflow_dir / "state.json"<br/> self.workflow_dir.mkdir(parents=True, exist_ok=True)<br/> <br/> def write(self, workflow: Workflow):<br/> """Write workflow state with automatic updatedAt timestamp"""<br/> workflow.updatedAt = datetime.now()<br/> try:<br/> self.state_file.write_text(workflow.model_dump_json(indent=2))<br/> logger.info(f"State written for workflow {self.workflow_id}")<br/> except Exception as e:<br/> logger.error(f"Failed to write state for {self.workflow_id}: {e}")<br/> raise<br/> <br/> def get(self) -> Workflow:<br/> """Read workflow state from disk"""<br/> if not self.state_file.exists():<br/> raise FileNotFoundError(f"State file not found for workflow {self.workflow_id}")<br/> try:<br/> return Workflow.model_validate_json(self.state_file.read_text())<br/> except Exception as e:<br/> logger.error(f"Failed to read state for {self.workflow_id}: {e}")<br/> raise<br/> <br/> def exists(self) -> bool:<br/> return self.state_file.exists()<br/>`|
|**File-Backed Queue**|`services/queue_service.py`|`python<br/># services/queue_service.py<br/>import json<br/>from pathlib import Path<br/>from typing import Dict, List, Optional<br/>from config import settings<br/>from loguru import logger<br/><br/>class MemoryQueue:<br/> def __init__(self, queue_file: Path = None):<br/> self.queue_file = queue_file or Path(settings.QUEUE_STATE_FILE)<br/> self.queue: List[Dict] = self._load()<br/> <br/> def _load(self) -> List[Dict]:<br/> """Load queue from disk, return empty list if file doesn't exist"""<br/> if not self.queue_file.exists():<br/> return []<br/> try:<br/> return json.loads(self.queue_file.read_text())<br/> except Exception as e:<br/> logger.error(f"Failed to load queue: {e}")<br/> return []<br/> <br/> def add(self, job: Dict):<br/> """Add job to queue. Job format: {'workflow_id': str, 'node_id': str}"""<br/> self.queue.append(job)<br/> self._persist()<br/> logger.info(f"Job added to queue: {job}")<br/> <br/> def get_next(self) -> Optional[Dict]:<br/> """Get and remove next job from queue"""<br/> if not self.queue:<br/> return None<br/> job = self.queue.pop(0)<br/> self._persist()<br/> logger.info(f"Job retrieved from queue: {job}")<br/> return job<br/> <br/> def _persist(self):<br/> """Persist queue to disk"""<br/> try:<br/> self.queue_file.write_text(json.dumps(self.queue, indent=2))<br/> except Exception as e:<br/> logger.error(f"Failed to persist queue: {e}")<br/> raise<br/> <br/> def size(self) -> int:<br/> return len(self.queue)<br/><br/># Global queue instance<br/>queue = MemoryQueue()<br/>`|
|**Workflow Creation Endpoint**|`app.py`|`python<br/># app.py (additions)<br/>from services.state_manager import StateManager<br/>from services.queue_service import queue<br/>from models.workflow import WorkflowStep<br/>import uuid<br/>from fastapi import HTTPException<br/><br/>def generate_workflow_id() -> str:<br/> return str(uuid.uuid4())<br/><br/>@router.post("/workflows/from-template")<br/>async def create_workflow(template_id: str, params: dict) -> Workflow:<br/> """Create workflow from template and queue initial nodes"""<br/> try:<br/> workflow_id = generate_workflow_id()<br/> <br/> # Create initial workflow step (simulated template)<br/> initial_step = WorkflowStep(<br/> id=f"step_{uuid.uuid4()}",<br/> name=f"Initial Step - {template_id}",<br/> action=template_id,<br/> status=JobStatus.PENDING,<br/> dependencies=[],<br/> logs=[]<br/> )<br/> <br/> # Create workflow<br/> workflow = Workflow(<br/> id=workflow_id,<br/> name=f"Workflow from {template_id}",<br/> status=JobStatus.PENDING,<br/> steps=[initial_step],<br/> createdAt=datetime.now(),<br/> updatedAt=datetime.now(),<br/> description=f"Generated from template: {template_id}",<br/> progress=0<br/> )<br/> <br/> # Persist initial state<br/> state_manager = StateManager(workflow_id)<br/> state_manager.write(workflow)<br/> <br/> # Queue nodes with no dependencies<br/> for step in workflow.steps:<br/> if not step.dependencies:<br/> queue.add({"workflow_id": workflow_id, "node_id": step.id})<br/> <br/> logger.info(f"Workflow created: {workflow_id}")<br/> return workflow<br/> <br/> except Exception as e:<br/> logger.error(f"Failed to create workflow: {e}")<br/> raise HTTPException(status_code=500, detail=f"Failed to create workflow: {str(e)}")<br/>`|
|**Background Worker**|`worker.py`|`python<br/># worker.py<br/>import asyncio<br/>from datetime import datetime<br/>from services.queue_service import queue<br/>from services.state_manager import StateManager<br/>from models.workflow import JobStatus<br/>from loguru import logger<br/>from config import settings<br/><br/>def calculate_duration(start_time: datetime, end_time: datetime) -> str:<br/> """Calculate human-readable duration string"""<br/> delta = end_time - start_time<br/> total_seconds = int(delta.total_seconds())<br/> <br/> if total_seconds < 60:<br/> return f"{total_seconds} sec"<br/> elif total_seconds < 3600:<br/> minutes = total_seconds // 60<br/> seconds = total_seconds % 60<br/> return f"{minutes} min {seconds} sec"<br/> else:<br/> hours = total_seconds // 3600<br/> minutes = (total_seconds % 3600) // 60<br/> return f"{hours} hr {minutes} min"<br/><br/>def calculate_workflow_progress(workflow) -> int:<br/> """Calculate workflow progress percentage"""<br/> if not workflow.steps:<br/> return 0<br/> <br/> completed_steps = len([s for s in workflow.steps if s.status == JobStatus.COMPLETED])<br/> total_steps = len(workflow.steps)<br/> return int((completed_steps / total_steps) * 100)<br/><br/>async def execute_node_simulation(node_id: str, action: str) -> tuple[dict, str]:<br/> """Simulate node execution with realistic timing"""<br/> await asyncio.sleep(5) # Simulate work<br/> <br/> # Simulate successful execution<br/> output = {<br/> "result": "success",<br/> "action": action,<br/> "processed_at": datetime.now().isoformat(),<br/> "node_id": node_id<br/> }<br/> <br/> return output, None # output, error<br/><br/>async def run_worker():<br/> """Main worker loop"""<br/> logger.info("Worker started")<br/> <br/> while True:<br/> try:<br/> job = queue.get_next()<br/> <br/> if job:<br/> workflow_id = job["workflow_id"]<br/> node_id = job["node_id"]<br/> <br/> logger.info(f"Processing job: {workflow_id}/{node_id}")<br/> <br/> # Load workflow state<br/> state_manager = StateManager(workflow_id)<br/> workflow = state_manager.get()<br/> <br/> # Find the node<br/> node = next((n for n in workflow.steps if n.id == node_id), None)<br/> if not node:<br/> logger.error(f"Node {node_id} not found in workflow {workflow_id}")<br/> continue<br/> <br/> # Update node to RUNNING<br/> node.status = JobStatus.RUNNING<br/> node.startTime = datetime.now()<br/> node.logs = node.logs or []<br/> node.logs.append(f"Started execution at {node.startTime.isoformat()}")<br/> workflow.status = JobStatus.RUNNING<br/> state_manager.write(workflow)<br/> <br/> # Execute the node<br/> try:<br/> output, error = await execute_node_simulation(node_id, node.action)<br/> <br/> # Update node with results<br/> node.endTime = datetime.now()<br/> node.duration = calculate_duration(node.startTime, node.endTime)<br/> <br/> if error:<br/> node.status = JobStatus.FAILED<br/> node.error = error<br/> node.logs.append(f"Failed: {error}")<br/> workflow.status = JobStatus.FAILED<br/> else:<br/> node.status = JobStatus.COMPLETED<br/> node.outputs = output<br/> node.logs.append(f"Completed successfully at {node.endTime.isoformat()}")<br/> <br/> # Update workflow progress<br/> workflow.progress = calculate_workflow_progress(workflow)<br/> <br/> # Check if entire workflow is complete<br/> if all(s.status == JobStatus.COMPLETED for s in workflow.steps):<br/> workflow.status = JobStatus.COMPLETED<br/> workflow.progress = 100<br/> <br/> # Persist final state<br/> state_manager.write(workflow)<br/> logger.info(f"Job completed: {workflow_id}/{node_id} -> {node.status}")<br/> <br/> except Exception as e:<br/> logger.error(f"Error executing node {node_id}: {e}")<br/> node.status = JobStatus.FAILED<br/> node.error = str(e)<br/> node.endTime = datetime.now()<br/> node.duration = calculate_duration(node.startTime, node.endTime)<br/> node.logs.append(f"Error: {str(e)}")<br/> workflow.status = JobStatus.FAILED<br/> state_manager.write(workflow)<br/> <br/> else:<br/> # No jobs available, brief pause<br/> await asyncio.sleep(1)<br/> <br/> except Exception as e:<br/> logger.error(f"Worker error: {e}")<br/> await asyncio.sleep(5) # Longer pause on error<br/>`|
|**Worker Integration**|`app.py` (modifications)|`python<br/># app.py (updated lifespan)<br/>from worker import run_worker<br/>import asyncio<br/>from loguru import logger<br/><br/>@asynccontextmanager<br/>async def lifespan(app: FastAPI):<br/> # Startup: Initialize worker<br/> logger.info("Starting Hybrid Engine Backend")<br/> worker_task = asyncio.create_task(run_worker())<br/> <br/> try:<br/> yield<br/> finally:<br/> # Shutdown: Cancel worker<br/> logger.info("Shutting down worker")<br/> worker_task.cancel()<br/> try:<br/> await worker_task<br/> except asyncio.CancelledError:<br/> pass<br/>`|
|**Additional Endpoints**|`app.py`|`python<br/># app.py (additional endpoints)<br/>@router.get("/workflows")<br/>async def get_workflows() -> List[Workflow]:<br/> """Get all workflows"""<br/> workflows = []<br/> workflows_dir = Path(settings.WORKFLOWS_DIR)<br/> <br/> if workflows_dir.exists():<br/> for workflow_dir in workflows_dir.iterdir():<br/> if workflow_dir.is_dir():<br/> try:<br/> state_manager = StateManager(workflow_dir.name)<br/> if state_manager.exists():<br/> workflow = state_manager.get()<br/> workflows.append(workflow)<br/> except Exception as e:<br/> logger.error(f"Failed to load workflow {workflow_dir.name}: {e}")<br/> <br/> return sorted(workflows, key=lambda w: w.updatedAt, reverse=True)<br/><br/>@router.get("/workflows/{workflow_id}")<br/>async def get_workflow_by_id(workflow_id: str) -> Workflow:<br/> """Get specific workflow by ID"""<br/> try:<br/> state_manager = StateManager(workflow_id)<br/> if not state_manager.exists():<br/> raise HTTPException(status_code=404, detail=f"Workflow with ID {workflow_id} not found")<br/> return state_manager.get()<br/> except FileNotFoundError:<br/> raise HTTPException(status_code=404, detail=f"Workflow with ID {workflow_id} not found")<br/> except Exception as e:<br/> logger.error(f"Error retrieving workflow {workflow_id}: {e}")<br/> raise HTTPException(status_code=500, detail="Internal server error")<br/>`|

**Success Criteria for Phase 1:**

- ✅ POST `/api/workflows/from-template` creates `workflows/{workflow_id}/state.json` and adds job to `queue-state.json`
- ✅ Background worker picks up jobs automatically and logs state transitions (PENDING → RUNNING → COMPLETED)
- ✅ `state.json` updates with correct status, calculated duration, log messages, and workflow progress
- ✅ Jobs are removed from `queue-state.json` upon completion
- ✅ GET `/api/workflows` returns list of all workflows with correct progress percentages
- ✅ GET `/api/workflows/{id}` returns specific workflow with all step details
- ✅ System handles worker crashes gracefully (jobs remain in queue for recovery)

## Implementation Notes

**Critical Success Patterns:**

1. **State Consistency**: Every state change goes through `StateManager.write()` to ensure the single source of truth is maintained.
    
2. **Error Boundaries**: Each component (queue, state manager, worker) has comprehensive error handling with logging.
    
3. **Progress Calculation**: The `calculate_workflow_progress()` function implements the exact contract specification: `(completed_steps / total_steps) * 100`.
    
4. **Duration Formatting**: Human-readable duration strings match frontend expectations ("1 min 30 sec", "2 hr 15 min").
    
5. **Recovery Ready**: The queue persists across restarts, enabling future recovery functionality.
    

This MVP delivers a complete, testable workflow execution system that satisfies all frontend contract requirements while maintaining the architectural principles of resilience and local-first operation.