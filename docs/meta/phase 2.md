# Phase 2 Implementation Plan: Advanced Orchestration

## 1. Phase 2 Objective

Transform the backend from a simple job runner into a true DAG-based orchestrator, enabling complex, multi-step workflows with dependency management, failure policies, and branching capabilities. This phase introduces intelligent failure isolation, atomic dependency resolution, and persistent workflow lineages that create an interconnected web of computational thought processes.

## 2. Implementation Steps

| Component | File(s) to Modify | Implementation Details & Key Code |
|-----------|-------------------|-----------------------------------|
| **Node Failure Policy** | `models/workflow.py`, `worker.py` | **Models (`models/workflow.py`)**: Add `on_failure` field to `WorkflowStep` class:<br/><br/>```python<br/>from typing import Literal<br/><br/>class WorkflowStep(BaseModel):<br/>    # ... existing fields ...<br/>    on_failure: Optional[Literal['stop_workflow', 'continue']] = 'stop_workflow'<br/>    params: Optional[Dict[str, Any]] = None<br/>```<br/><br/>**Worker (`worker.py`)**: Implement failure policy in the exception handling block:<br/><br/>```python<br/># In the except Exception as e block:<br/>node.status = JobStatus.FAILED<br/>node.error = str(e)<br/>node.endTime = datetime.now()<br/>node.duration = calculate_duration(node.startTime, node.endTime)<br/>node.logs.append(f"Failed: {str(e)}")<br/><br/># NEW: on_failure handling<br/>if node.on_failure == 'stop_workflow':<br/>    workflow.status = JobStatus.FAILED<br/>    logger.warning(f"Workflow {workflow_id} stopped due to failed step {node_id}")<br/>    # Prevent dependent nodes from being queued<br/>    for step in workflow.steps:<br/>        if step.status == JobStatus.PENDING and node_id in (step.dependencies or []):<br/>            step.status = JobStatus.STOPPED<br/>else:  # 'continue'<br/>    # Check if workflow can still complete<br/>    remaining_pending = [s for s in workflow.steps if s.status == JobStatus.PENDING]<br/>    if not remaining_pending or all(s.status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.STOPPED] for s in workflow.steps):<br/>        workflow.status = JobStatus.COMPLETED<br/>        workflow.progress = 100<br/><br/>state_manager.write(workflow)<br/>```|
| **DAG Execution & Dependent Node Queuing** | `worker.py` | **Worker (`worker.py`)**: Add dependency checking before execution and dependent node queuing after completion:<br/><br/>```python<br/># BEFORE executing node - add dependency validation:<br/>if node.dependencies:<br/>    completed_step_ids = {s.id for s in workflow.steps if s.status == JobStatus.COMPLETED}<br/>    if not set(node.dependencies).issubset(completed_step_ids):<br/>        logger.warning(f"Dependencies not met for {workflow_id}/{node_id}. Skipping.")<br/>        continue<br/><br/># AFTER successful completion - add dependent node queuing:<br/>if node.status == JobStatus.COMPLETED:<br/>    # ... existing completion logic ...<br/>    <br/>    # NEW: Queue dependent nodes<br/>    completed_step_ids = {s.id for s in workflow.steps if s.status == JobStatus.COMPLETED}<br/>    for next_step in workflow.steps:<br/>        if (next_step.status == JobStatus.PENDING and <br/>            next_step.dependencies and <br/>            set(next_step.dependencies).issubset(completed_step_ids)):<br/>            <br/>            # Prevent duplicate queuing<br/>            job_key = f"{workflow_id}/{next_step.id}"<br/>            if not any(f"{j['workflow_id']}/{j['node_id']}" == job_key for j in queue.queue):<br/>                queue.add({"workflow_id": workflow_id, "node_id": next_step.id})<br/>                next_step.status = JobStatus.WAITING_FOR_DEPENDENCY<br/>                logger.info(f"Queued dependent step {next_step.id} for workflow {workflow_id}")<br/>    <br/>    # Determine final workflow status<br/>    if all(s.status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.STOPPED] for s in workflow.steps):<br/>        failed_critical = any(s.status == JobStatus.FAILED and s.on_failure == 'stop_workflow' for s in workflow.steps)<br/>        workflow.status = JobStatus.FAILED if failed_critical else JobStatus.COMPLETED<br/>        workflow.progress = 100<br/>    <br/>    state_manager.write(workflow)<br/>```|
| **Workflow Branching API** | `app.py` | **API (`app.py`)**: Add complete branching endpoint:<br/><br/>```python<br/>import shutil<br/>from fastapi import Body<br/><br/>@router.post("/workflows/{workflow_id}/branch")<br/>async def create_workflow_branch(<br/>    workflow_id: str, <br/>    branch_name: str = Body(..., embed=True)<br/>) -> Workflow:<br/>    """Create a new branch from an existing workflow"""<br/>    try:<br/>        # 1. Load parent workflow<br/>        parent_sm = StateManager(workflow_id)<br/>        if not parent_sm.exists():<br/>            raise HTTPException(status_code=404, detail="Parent workflow not found")<br/>        parent_workflow = parent_sm.get()<br/>        <br/>        # 2. Create new ID and directory structure<br/>        new_id = generate_workflow_id()<br/>        new_sm = StateManager(new_id)<br/>        shutil.copytree(parent_sm.workflow_dir, new_sm.workflow_dir)<br/>        <br/>        # 3. Deep copy and reset workflow state<br/>        branched_workflow = new_sm.get()<br/>        branched_workflow.id = new_id<br/>        branched_workflow.name = branch_name or f"{parent_workflow.name} (Branch)"<br/>        branched_workflow.parentId = workflow_id<br/>        branched_workflow.status = JobStatus.PENDING<br/>        branched_workflow.progress = 0<br/>        branched_workflow.branches = []<br/>        branched_workflow.createdAt = datetime.now()<br/>        branched_workflow.updatedAt = datetime.now()<br/>        <br/>        # Reset all step statuses<br/>        for step in branched_workflow.steps:<br/>            step.status = JobStatus.PENDING<br/>            step.startTime = step.endTime = step.duration = step.outputs = step.error = None<br/>            step.logs = [f"Branched from {workflow_id} at {datetime.now().isoformat()}"]<br/>        <br/>        new_sm.write(branched_workflow)<br/>        <br/>        # 4. Update parent's branches list<br/>        if parent_workflow.branches is None:<br/>            parent_workflow.branches = []<br/>        parent_workflow.branches.append({"id": new_id, "name": branched_workflow.name})<br/>        parent_workflow.updatedAt = datetime.now()<br/>        parent_sm.write(parent_workflow)<br/>        <br/>        # 5. Auto-queue initial nodes (no dependencies)<br/>        for step in branched_workflow.steps:<br/>            if not step.dependencies:<br/>                queue.add({"workflow_id": new_id, "node_id": step.id})<br/>        <br/>        logger.info(f"Created branch {new_id} from {workflow_id}")<br/>        return branched_workflow<br/>        <br/>    except Exception as e:<br/>        logger.error(f"Failed to branch workflow {workflow_id}: {e}")<br/>        raise HTTPException(status_code=500, detail=f"Failed to create branch: {str(e)}")<br/>```|

## 3. Key Architectural Benefits

- **Atomic State Management**: All state changes are persisted in single write operations using `state_manager.write(workflow)`, ensuring consistency during concurrent operations and system failures.

- **Failure Isolation**: The `on_failure` policy creates isolated failure domains where individual node failures can either halt the entire workflow (`stop_workflow`) or allow continued execution (`continue`), preventing cascading failures.

- **Queue Integrity**: Duplicate job queuing is prevented through explicit job key checking, ensuring each node executes exactly once while maintaining dependency order.

- **Persistent Workflow Lineages**: Branching creates auditable, exploratory paths with `parentId` relationships and bidirectional branch tracking, enabling complex workflow evolution patterns.

## 4. Validation Checklist

| Test Case | Verification Method |
|-----------|-------------------|
| **Dependency Ordering (A→B→C)** | Create workflow with linear dependencies, verify B only starts after A completes, C only starts after B completes |
| **Multi-dependency (D depends on B and C)** | Create workflow where D has dependencies: [B, C], verify D only executes after both B and C are COMPLETED |
| **Failure Propagation (stop_workflow policy)** | Set node with `on_failure: 'stop_workflow'`, trigger failure, verify workflow status becomes FAILED and dependent nodes become STOPPED |
| **Continue on Failure (continue policy)** | Set node with `on_failure: 'continue'`, trigger failure, verify workflow continues and dependent nodes can still execute |
| **Branch Isolation (parent modification doesn't affect branch)** | Create branch, modify parent workflow state, verify branch maintains independent state and execution |
| **Auto-queuing of Branched Nodes** | Create branch from workflow, verify initial nodes (no dependencies) are automatically queued for execution in the new branch |