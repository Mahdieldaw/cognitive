# Phase 3 Implementation Plan: Enhancements & Extensibility

## 1. Phase 3 Objective

Phase 3 transforms the Hybrid Engine from a sophisticated DAG orchestrator into a production-ready AI workflow platform that bridges local execution with external data sources. This phase implements real-world AI model integration with comprehensive metrics tracking, establishes system resilience through automated recovery mechanisms, and extends the engine's reach via browser integration. The objective is to create a robust, observable, and extensible system that maintains its local-first principles while seamlessly integrating with the broader digital ecosystem.

## 2. Implementation Steps

| Component | File(s) to Create/Modify | Implementation Details & Key Code |
|-----------|-------------------------|----------------------------------|
| **Real Model Adapter & Metrics Aggregation** | `backend/models/workflow.py`, `backend/worker.py`, `backend/adapters/openai_adapter.py` (New), `backend/adapters/base_adapter.py` (New) | **Models Enhancement (`backend/models/workflow.py`)**: Extend data models to capture execution metrics and metadata.<br/><br/>```python<br/>class WorkflowStep(BaseModel):<br/>    # ... existing fields ...<br/>    metadata: Optional[Dict[str, Any]] = None  # Model execution metadata<br/>    execution_metrics: Optional[Dict[str, Any]] = None  # Detailed metrics<br/><br/>class Workflow(BaseModel):<br/>    # ... existing fields ...<br/>    metrics: Optional[Dict[str, Any]] = None  # Aggregated workflow metrics<br/>    cost_breakdown: Optional[Dict[str, float]] = None  # Cost per model/step<br/>```<br/><br/>**Base Adapter (`backend/adapters/base_adapter.py`)**: Define adapter interface for consistency.<br/><br/>```python<br/>from abc import ABC, abstractmethod<br/>from typing import Dict, Any, Optional<br/><br/>class BaseAdapter(ABC):<br/>    @abstractmethod<br/>    async def execute(self, prompt: str, **kwargs) -> Dict[str, Any]:<br/>        """Execute model call and return standardized response."""<br/>        pass<br/><br/>    def calculate_cost(self, tokens: int, model: str) -> float:<br/>        """Calculate cost based on token count and model."""<br/>        # Implement cost calculation logic<br/>        pass<br/>```<br/><br/>**OpenAI Adapter (`backend/adapters/openai_adapter.py`)**: Production-ready OpenAI integration.<br/><br/>```python<br/>import openai<br/>from .base_adapter import BaseAdapter<br/>from datetime import datetime<br/>import asyncio<br/><br/>class OpenAIAdapter(BaseAdapter):<br/>    def __init__(self, api_key: str):<br/>        self.client = openai.AsyncOpenAI(api_key=api_key)<br/>    <br/>    async def execute(self, prompt: str, model: str = "gpt-4", **kwargs) -> Dict[str, Any]:<br/>        try:<br/>            start_time = datetime.now()<br/>            response = await self.client.chat.completions.create(<br/>                model=model,<br/>                messages=[{"role": "user", "content": prompt}],<br/>                **kwargs<br/>            )<br/>            end_time = datetime.now()<br/>            <br/>            usage = response.usage<br/>            cost = self.calculate_cost(usage.total_tokens, model)<br/>            <br/>            return {<br/>                "output": response.choices[0].message.content,<br/>                "error": None,<br/>                "metadata": {<br/>                    "model": model,<br/>                    "tokens": usage.total_tokens,<br/>                    "prompt_tokens": usage.prompt_tokens,<br/>                    "completion_tokens": usage.completion_tokens,<br/>                    "cost": cost,<br/>                    "duration_ms": (end_time - start_time).total_seconds() * 1000,<br/>                    "timestamp": start_time.isoformat()<br/>                }<br/>            }<br/>        except Exception as e:<br/>            return {<br/>                "output": None,<br/>                "error": str(e),<br/>                "metadata": {"model": model, "error_type": type(e).__name__}<br/>            }<br/>```<br/><br/>**Worker Enhancement (`backend/worker.py`)**: Replace simulation with real adapter routing and metrics aggregation.<br/><br/>```python<br/># Add imports<br/>from backend.adapters.openai_adapter import OpenAIAdapter<br/>from backend.adapters.deepseek_adapter import DeepSeekAdapter<br/>from backend.adapters.gemini_adapter import GeminiAdapter<br/><br/># Initialize adapters<br/>adapters = {<br/>    "openai_chat": OpenAIAdapter(settings.OPENAI_API_KEY),<br/>    "deepseek_chat": DeepSeekAdapter(settings.DEEPSEEK_API_KEY),<br/>    "gemini_chat": GeminiAdapter(settings.GOOGLE_API_KEY)<br/>}<br/><br/>async def execute_node(node_id: str, action: str, params: dict) -> tuple[any, str | None, dict | None]:<br/>    """Execute node using appropriate adapter."""<br/>    logger.info(f"Executing node {node_id} with action '{action}'")<br/>    <br/>    if action in adapters:<br/>        adapter = adapters[action]<br/>        prompt = params.get("prompt", "")<br/>        model_params = {k: v for k, v in params.items() if k != "prompt"}<br/>        <br/>        result = await adapter.execute(prompt, **model_params)<br/>        return result.get("output"), result.get("error"), result.get("metadata")<br/>    else:<br/>        # Fallback for unknown actions<br/>        logger.warning(f"Unknown action: {action}. Using simulation.")<br/>        await asyncio.sleep(1)<br/>        return {"simulated": True}, None, {"action": action}<br/><br/># In run_worker(), after node completion:<br/>if not error:<br/>    node.metadata = metadata<br/>    node.execution_metrics = {<br/>        "tokens": metadata.get("tokens", 0),<br/>        "cost": metadata.get("cost", 0.0),<br/>        "model": metadata.get("model", "unknown")<br/>    }<br/>    <br/>    # Aggregate workflow metrics<br/>    total_tokens = sum(<br/>        s.execution_metrics.get("tokens", 0) <br/>        for s in workflow.steps if s.execution_metrics<br/>    )<br/>    total_cost = sum(<br/>        s.execution_metrics.get("cost", 0.0) <br/>        for s in workflow.steps if s.execution_metrics<br/>    )<br/>    <br/>    workflow.metrics = {<br/>        "total_tokens": total_tokens,<br/>        "total_cost": total_cost,<br/>        "completed_steps": len([s for s in workflow.steps if s.status == JobStatus.COMPLETED])<br/>    }<br/>    <br/>    # Cost breakdown by model<br/>    cost_by_model = {}<br/>    for step in workflow.steps:<br/>        if step.execution_metrics and "model" in step.execution_metrics:<br/>            model = step.execution_metrics["model"]<br/>            cost_by_model[model] = cost_by_model.get(model, 0.0) + step.execution_metrics.get("cost", 0.0)<br/>    workflow.cost_breakdown = cost_by_model<br/>```|
| **Recovery Manager for Orphaned Workflows** | `backend/recovery_manager.py` (New), `backend/app.py` | **Recovery Manager (`backend/recovery_manager.py`)**: Implement comprehensive orphan detection and recovery.<br/><br/>```python<br/>import os<br/>from pathlib import Path<br/>from loguru import logger<br/>from services.state_manager import StateManager<br/>from services.queue_service import queue<br/>from models.workflow import JobStatus<br/>from config import settings<br/>from datetime import datetime<br/><br/>class RecoveryManager:<br/>    def __init__(self):<br/>        self.workflows_dir = Path(settings.WORKFLOWS_DIR)<br/>    <br/>    async def check_and_recover_orphans(self):<br/>        """Scan for orphaned workflows and recover them."""<br/>        logger.info("Starting orphan workflow recovery check...")<br/>        <br/>        if not self.workflows_dir.exists():<br/>            logger.info("No workflows directory found. No recovery needed.")<br/>            return<br/>        <br/>        recovered_workflows = 0<br/>        recovered_steps = 0<br/>        <br/>        for workflow_dir in self.workflows_dir.iterdir():<br/>            if not workflow_dir.is_dir():<br/>                continue<br/>            <br/>            try:<br/>                workflow_id = workflow_dir.name<br/>                state_manager = StateManager(workflow_id)<br/>                <br/>                if not state_manager.exists():<br/>                    logger.warning(f"Workflow directory {workflow_id} has no state.json")<br/>                    continue<br/>                <br/>                workflow = state_manager.get()<br/>                <br/>                # Check if workflow is orphaned (status RUNNING but no active jobs)<br/>                if workflow.status == JobStatus.RUNNING:<br/>                    logger.info(f"Found potentially orphaned workflow: {workflow_id}")<br/>                    <br/>                    # Reset workflow status to PENDING for re-evaluation<br/>                    workflow.status = JobStatus.PENDING<br/>                    workflow.updatedAt = datetime.now()<br/>                    <br/>                    # Find and recover orphaned steps<br/>                    steps_to_requeue = []<br/>                    completed_step_ids = {s.id for s in workflow.steps if s.status == JobStatus.COMPLETED}<br/>                    <br/>                    for step in workflow.steps:<br/>                        # Recover RUNNING steps (were executing when crash occurred)<br/>                        if step.status == JobStatus.RUNNING:<br/>                            step.status = JobStatus.PENDING<br/>                            step.error = None<br/>                            step.endTime = None<br/>                            step.logs = step.logs or []<br/>                            step.logs.append(f"Recovered from orphaned RUNNING state at {datetime.now().isoformat()}")<br/>                            logger.info(f"Reset orphaned RUNNING step: {step.id}")<br/>                        <br/>                        # Recover WAITING_FOR_DEPENDENCY steps (were queued when crash occurred)<br/>                        elif step.status == JobStatus.WAITING_FOR_DEPENDENCY:<br/>                            step.status = JobStatus.PENDING<br/>                            step.logs = step.logs or []<br/>                            step.logs.append(f"Recovered from orphaned WAITING_FOR_DEPENDENCY state at {datetime.now().isoformat()}")<br/>                        <br/>                        # Re-queue steps that are now PENDING and have dependencies met<br/>                        if step.status == JobStatus.PENDING:<br/>                            if not step.dependencies or set(step.dependencies).issubset(completed_step_ids):<br/>                                steps_to_requeue.append(step.id)<br/>                                step.status = JobStatus.WAITING_FOR_DEPENDENCY<br/>                                step.logs = step.logs or []<br/>                                step.logs.append(f"Re-queued during recovery at {datetime.now().isoformat()}")<br/>                    <br/>                    # Add recovered steps to queue<br/>                    for step_id in steps_to_requeue:<br/>                        queue.add({"workflow_id": workflow_id, "node_id": step_id})<br/>                        recovered_steps += 1<br/>                    <br/>                    # Update workflow progress<br/>                    completed_count = len([s for s in workflow.steps if s.status == JobStatus.COMPLETED])<br/>                    workflow.progress = int((completed_count / len(workflow.steps)) * 100) if workflow.steps else 0<br/>                    <br/>                    # Save recovered workflow<br/>                    state_manager.write(workflow)<br/>                    <br/>                    recovered_workflows += 1<br/>                    logger.info(f"Recovered workflow {workflow_id}: {len(steps_to_requeue)} steps re-queued")<br/>            <br/>            except Exception as e:<br/>                logger.error(f"Error recovering workflow {workflow_dir.name}: {e}")<br/>        <br/>        logger.info(f"Recovery complete: {recovered_workflows} workflows, {recovered_steps} steps recovered")<br/>    <br/>    async def cleanup_stale_queue_items(self):<br/>        """Remove queue items for workflows that no longer exist."""<br/>        # Implementation for cleaning up queue items referencing deleted workflows<br/>        pass<br/>```<br/><br/>**App Integration (`backend/app.py`)**: Integrate recovery manager into application lifecycle.<br/><br/>```python<br/># Add import<br/>from recovery_manager import RecoveryManager<br/><br/>@asynccontextmanager<br/>async def lifespan(app: FastAPI):<br/>    # Startup: Run recovery check then start worker<br/>    logger.info("Starting Hybrid Engine Backend")<br/>    <br/>    # Initialize and run recovery<br/>    recovery_manager = RecoveryManager()<br/>    await recovery_manager.check_and_recover_orphans()<br/>    <br/>    # Start worker after recovery<br/>    worker_task = asyncio.create_task(run_worker())<br/>    <br/>    try:<br/>        yield<br/>    finally:<br/>        # Shutdown: Cancel worker<br/>        logger.info("Shutting down worker")<br/>        worker_task.cancel()<br/>        try:<br/>            await worker_task<br/>        except asyncio.CancelledError:<br/>            pass<br/>```|
| **Browser Integration API Endpoint** | `backend/app.py`, `backend/models/external_data.py` (New) | **External Data Model (`backend/models/external_data.py`)**: Define schema for browser-captured data.<br/><br/>```python<br/>from pydantic import BaseModel, HttpUrl<br/>from typing import Optional, Dict, Any<br/>from datetime import datetime<br/><br/>class ExternalDataRequest(BaseModel):<br/>    content: str<br/>    source_url: Optional[HttpUrl] = None<br/>    content_type: str = "text"  # text, html, json, etc.<br/>    metadata: Optional[Dict[str, Any]] = None<br/>    step_name: Optional[str] = None<br/>    action_type: str = "external_data"<br/><br/>class ExternalDataResponse(BaseModel):<br/>    step_id: str<br/>    workflow_id: str<br/>    queued_dependents: int<br/>    status: str<br/>```<br/><br/>**API Endpoint Implementation (`backend/app.py`)**: Add endpoint for external data ingestion.<br/><br/>```python<br/># Add imports<br/>from models.external_data import ExternalDataRequest, ExternalDataResponse<br/>import uuid<br/><br/>@router.post("/workflows/{workflow_id}/external-data")<br/>async def add_external_data(workflow_id: str, request: ExternalDataRequest) -> ExternalDataResponse:<br/>    """Add external data to workflow and trigger dependent steps."""<br/>    try:<br/>        # Load target workflow<br/>        state_manager = StateManager(workflow_id)<br/>        if not state_manager.exists():<br/>            raise HTTPException(status_code=404, detail=f"Workflow {workflow_id} not found")<br/>        <br/>        workflow = state_manager.get()<br/>        <br/>        # Create new step for external data<br/>        step_id = f"ext_{uuid.uuid4().hex[:8]}"<br/>        current_time = datetime.now()<br/>        <br/>        external_step = WorkflowStep(<br/>            id=step_id,<br/>            name=request.step_name or f"External Data from {request.source_url or 'Browser'}",<br/>            action=request.action_type,<br/>            status=JobStatus.COMPLETED,  # Immediately completed<br/>            dependencies=[],  # External data has no dependencies<br/>            outputs={<br/>                "content": request.content,<br/>                "source_url": str(request.source_url) if request.source_url else None,<br/>                "content_type": request.content_type,<br/>                "captured_at": current_time.isoformat()<br/>            },<br/>            startTime=current_time,<br/>            endTime=current_time,<br/>            duration="0 sec",<br/>            logs=[f"External data ingested at {current_time.isoformat()}"],<br/>            metadata=request.metadata or {},<br/>            params={"source": "browser_extension"}<br/>        )<br/>        <br/>        # Add step to workflow<br/>        workflow.steps.append(external_step)<br/>        workflow.updatedAt = current_time<br/>        <br/>        # Find and queue dependent steps<br/>        completed_step_ids = {s.id for s in workflow.steps if s.status == JobStatus.COMPLETED}<br/>        queued_dependents = 0<br/>        <br/>        for step in workflow.steps:<br/>            if (step.status == JobStatus.PENDING and <br/>                step.dependencies and <br/>                set(step.dependencies).issubset(completed_step_ids)):<br/>                <br/>                # Check if already queued<br/>                job_key = f"{workflow_id}/{step.id}"<br/>                is_queued = any(<br/>                    f"{j['workflow_id']}/{j['node_id']}" == job_key <br/>                    for j in queue.queue if hasattr(queue, 'queue')<br/>                )<br/>                <br/>                if not is_queued:<br/>                    queue.add({"workflow_id": workflow_id, "node_id": step.id})<br/>                    step.status = JobStatus.WAITING_FOR_DEPENDENCY<br/>                    step.logs = step.logs or []<br/>                    step.logs.append(f"Queued due to external data dependency satisfaction at {current_time.isoformat()}")<br/>                    queued_dependents += 1<br/>        <br/>        # Update workflow progress<br/>        completed_count = len([s for s in workflow.steps if s.status == JobStatus.COMPLETED])<br/>        workflow.progress = int((completed_count / len(workflow.steps)) * 100) if workflow.steps else 0<br/>        <br/>        # Set workflow status to RUNNING if it was PENDING and now has queued items<br/>        if workflow.status == JobStatus.PENDING and queued_dependents > 0:<br/>            workflow.status = JobStatus.RUNNING<br/>        <br/>        # Save updated workflow<br/>        state_manager.write(workflow)<br/>        <br/>        logger.info(f"External data added to workflow {workflow_id}: step {step_id}, {queued_dependents} dependents queued")<br/>        <br/>        return ExternalDataResponse(<br/>            step_id=step_id,<br/>            workflow_id=workflow_id,<br/>            queued_dependents=queued_dependents,<br/>            status="success"<br/>        )<br/>    <br/>    except HTTPException as http_exc:<br/>        raise http_exc<br/>    except Exception as e:<br/>        logger.error(f"Error adding external data to workflow {workflow_id}: {e}", exc_info=True)<br/>        raise HTTPException(status_code=500, detail=f"Failed to add external data: {str(e)}")<br/><br/>@router.get("/workflows/{workflow_id}/external-data")<br/>async def get_external_data_steps(workflow_id: str):<br/>    """Get all external data steps for a workflow."""<br/>    try:<br/>        state_manager = StateManager(workflow_id)<br/>        if not state_manager.exists():<br/>            raise HTTPException(status_code=404, detail=f"Workflow {workflow_id} not found")<br/>        <br/>        workflow = state_manager.get()<br/>        external_steps = [<br/>            step for step in workflow.steps <br/>            if step.action == "external_data" or step.params.get("source") == "browser_extension"<br/>        ]<br/>        <br/>        return {<br/>            "workflow_id": workflow_id,<br/>            "external_steps": external_steps,<br/>            "count": len(external_steps)<br/>        }<br/>    except Exception as e:<br/>        logger.error(f"Error retrieving external data steps: {e}")<br/>        raise HTTPException(status_code=500, detail="Internal server error")<br/>```|

## 3. Key Architectural Benefits

The Phase 3 enhancements provide critical architectural value:

**Extensibility**: The adapter pattern with `BaseAdapter` creates a standardized interface for integrating any AI model or external service. New models require only implementing the `execute` method, making the system infinitely expandable without core modifications.

**Observability**: Comprehensive metrics tracking at both step and workflow levels provides visibility into cost, performance, and resource utilization. This data enables optimization, budgeting, and performance analysis across different models and workflow patterns.

**Resilience**: The Recovery Manager ensures zero workflow loss during system failures. Orphaned workflows are automatically detected and resumed, maintaining the system's reliability promise even in unstable environments.

**Interoperability**: The external data API transforms the engine from an isolated system into a connected orchestrator. Browser integration is just the beginningâ€”this endpoint can accept data from any external source, making workflows truly hybrid.

**Production Readiness**: Real model integrations with proper error handling, cost tracking, and retry logic prepare the system for production deployment at scale.

## 4. Validation Checklist

| Test Case | Verification Method |
|-----------|-------------------|
| **Metrics Tracking** | Execute a workflow with OpenAI/DeepSeek steps. Verify `state.json` contains populated `metadata`, `execution_metrics` at step level, and `metrics`/`cost_breakdown` at workflow level. Confirm token counts and costs are accurate. |
| **Recovery Manager** | Stop the server while a workflow is RUNNING. Restart and check logs for orphan detection. Verify orphaned steps are reset to PENDING, re-queued appropriately, and workflow execution resumes from the correct point. |
| **Browser Integration Backend** | Use curl/Postman to POST external data to `/workflows/{id}/external-data`. Verify: (1) New COMPLETED step appears in `state.json`, (2) Dependent steps are queued in `queue-state.json`, (3) Workflow status updates to RUNNING if applicable. |
| **Adapter Routing** | Create workflows with different action types (`openai_chat`, `deepseek_chat`, `gemini_chat`). Verify each routes to the correct adapter and returns model-specific metadata. |
| **Cost Aggregation** | Run multi-step workflow with different models. Verify `workflow.cost_breakdown` accurately reflects costs per model and `total_cost` matches sum of individual step costs. |
| **Dependency Satisfaction** | Create workflow where step B depends on external data step A. Add external data via API and verify step B is immediately queued and executed. |